{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "Sakshi Lathi, 1234549838\n",
        "\n",
        "# STaR: Self-Taught Reasoner Implementation\n",
        "\n",
        "## Bootstrapping Logical Reasoning Through Self-Improvement\n",
        "\n",
        "**Model Used:** Llama-3.2-3B-Instruct\n",
        "**Dataset:** GSM8K (training and evaluation sets)\n",
        "**Techniques Applied:**\n",
        "\n",
        "* Zero-Shot Chain-of-Thought (CoT)\n",
        "* Vanilla Supervised Fine-Tuning (SFT)\n",
        "* STaR (Self-Taught Reasoner)\n",
        "\n",
        "---\n",
        "\n",
        "This project focuses on reproducing the **STaR: Self-Taught Reasoner** approach, which enhances a model's reasoning ability by iteratively generating and refining rationales. Using **Llama-3.2-3B-Instruct**, we experiment with three paradigms: a baseline zero-shot reasoning model, a supervised fine-tuning setup using provided rationales, and the STaR method, where the model teaches itself through bootstrapped rationales derived from its own outputs. The GSM8K dataset serves as both the training corpus for rationale generation and the benchmark for final evaluation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "# Installing required packages\n",
        "!pip install -q datasets transformers torch accelerate peft bitsandbytes tqdm\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9Os4-9yRz_x"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import re\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from typing import List, Dict, Tuple\n",
        "import numpy as np\n",
        "\n",
        "# Setting random seeds\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config_code"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "class Config:\n",
        "    # Model and dataset\n",
        "    MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "    DATASET_NAME = \"openai/gsm8k\"\n",
        "    DATASET_CONFIG = \"main\"\n",
        "\n",
        "    # Training parameters\n",
        "    BATCH_SIZE = 2 # batch size is reduced\n",
        "    LEARNING_RATE = 2e-5\n",
        "    NUM_EPOCHS = 3\n",
        "    MAX_LENGTH = 512\n",
        "    GRADIENT_ACCUMULATION_STEPS = 16 # gradient accumulation is increased\n",
        "    WARMUP_STEPS = 100\n",
        "\n",
        "    # STaR specific\n",
        "    STAR_ITERATIONS = 1\n",
        "    MAX_NEW_TOKENS = 400\n",
        "    TEMPERATURE = 0.7\n",
        "\n",
        "    # Testing\n",
        "    TRAIN_SAMPLE_SIZE = 200\n",
        "    TEST_SAMPLE_SIZE = 50\n",
        "\n",
        "    # Output directories\n",
        "    OUTPUT_DIR = \"home/slathi//star_output\"\n",
        "    VANILLA_SFT_DIR = \"home/slathi//vanilla_sft_output\"\n",
        "    STAR_DIR = \"home/slathi//star_iterations\"\n",
        "    DATA_DIR = \"home/slathi//star_data\"\n",
        "config = Config()\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(config.VANILLA_SFT_DIR, exist_ok=True)\n",
        "os.makedirs(config.STAR_DIR, exist_ok=True)\n",
        "os.makedirs(config.DATA_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Model: {config.MODEL_NAME}\")\n",
        "print(f\"  Dataset: {config.DATASET_NAME}\")\n",
        "print(f\"  STaR Iterations: {config.STAR_ITERATIONS}\")\n",
        "print(f\"  Training Epochs: {config.NUM_EPOCHS}\")\n",
        "print(f\"  Train Sample Size: {config.TRAIN_SAMPLE_SIZE}\") # Added train sample size to print\n",
        "print(f\"  Test Sample Size: {config.TEST_SAMPLE_SIZE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prompts"
      },
      "source": [
        "## Prompts\n",
        "\n",
        "### Rationale Generation Prompt (without hint):\n",
        "```\n",
        "Question: {question}\n",
        "Let's think step by step to solve this problem.\n",
        "```\n",
        "\n",
        "### Rationale Generation with Hint (rationalization):\n",
        "```\n",
        "Question: {question}\n",
        "The answer is {correct_answer}. Let's think step by step to explain how we get this answer.\n",
        "```\n",
        "\n",
        "### Zero-Shot CoT Prompt:\n",
        "```\n",
        "Question: {question}\n",
        "Let's think step by step.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prompt_templates"
      },
      "outputs": [],
      "source": [
        "# Prompt templates\n",
        "PROMPT_WITHOUT_HINT = \"\"\"Question: {question}\n",
        "Let's think step by step to solve this problem.\"\"\"\n",
        "\n",
        "PROMPT_WITH_HINT = \"\"\"Question: {question}\n",
        "The answer is {answer}. Let's think step by step to explain how we get this answer.\"\"\"\n",
        "\n",
        "ZERO_SHOT_COT_PROMPT = \"\"\"Question: {question}\n",
        "Let's think step by step.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc7ce0a7"
      },
      "outputs": [],
      "source": [
        "# Log in to Hugging Face\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "print(\"Logging in to Hugging Face...\")\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_dataset"
      },
      "outputs": [],
      "source": [
        "# Load GSM8K dataset\n",
        "print(\"Loading GSM8K dataset...\")\n",
        "dataset = load_dataset(config.DATASET_NAME, config.DATASET_CONFIG)\n",
        "train_dataset = dataset['train']\n",
        "test_dataset = dataset['test']\n",
        "\n",
        "# Sample for testing if specified\n",
        "if config.TRAIN_SAMPLE_SIZE:\n",
        "    train_dataset = train_dataset.select(range(config.TRAIN_SAMPLE_SIZE))\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)}\")\n",
        "print(f\"Test size: {len(test_dataset)}\")\n",
        "print(f\"\\nExample from train set:\")\n",
        "print(f\"Question: {train_dataset[0]['question']}\")\n",
        "print(f\"Answer: {train_dataset[0]['answer']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_model"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer and model\n",
        "print(\"Loading tokenizer and model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Load model with 8-bit quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    load_in_8bit=True\n",
        ")\n",
        "\n",
        "print(\"Model loaded successfully!\")\n",
        "print(f\"Device: {model.device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utility_functions"
      },
      "outputs": [],
      "source": [
        "# Utility Functions\n",
        "def extract_answer(text: str) -> str:\n",
        "\n",
        "    # GSM8K format: answer is after ####\n",
        "    if '####' in text:\n",
        "        answer = text.split('####')[-1].strip()\n",
        "        # Extract just the number\n",
        "        numbers = re.findall(r'-?\\d+\\.?\\d*', answer)\n",
        "        if numbers:\n",
        "            return numbers[0]\n",
        "\n",
        "    # Try to extract from common patterns\n",
        "    patterns = [\n",
        "        r'(?:the answer is|answer:|final answer is|answer =)\\s*\\$?\\s*(-?\\d+\\.?\\d*)',\n",
        "        r'\\$\\s*(-?\\d+\\.?\\d*)',\n",
        "        r'(-?\\d+\\.?\\d*)\\s*$',\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        matches = re.findall(pattern, text.lower())\n",
        "        if matches:\n",
        "            return matches[-1]\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "def check_answer_correctness(predicted: str, ground_truth: str) -> bool:\n",
        "\n",
        "    pred_answer = extract_answer(predicted)\n",
        "    gt_answer = extract_answer(ground_truth)\n",
        "\n",
        "    if not pred_answer or not gt_answer:\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        return float(pred_answer) == float(gt_answer)\n",
        "    except ValueError:\n",
        "        return pred_answer == gt_answer\n",
        "\n",
        "def generate_response(model, tokenizer, prompt: str, max_new_tokens: int = 400) -> str:\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=config.MAX_LENGTH)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=config.TEMPERATURE,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "    return response.strip()\n",
        "\n",
        "# Test the functions\n",
        "test_text = \"So the total is 25 + 10 = 35. #### 35\"\n",
        "print(f\"Test extract_answer: '{extract_answer(test_text)}'\")\n",
        "print(f\"Test correctness check: {check_answer_correctness(test_text, '#### 35')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zero_shot_eval"
      },
      "outputs": [],
      "source": [
        "def evaluate_zero_shot_cot(model, tokenizer, test_data, num_samples=None):\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Evaluating Zero-Shot CoT...\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    if num_samples:\n",
        "        test_data = test_data.select(range(min(num_samples, len(test_data))))\n",
        "\n",
        "    correct = 0\n",
        "    total = len(test_data)\n",
        "    results = []\n",
        "\n",
        "    for i, example in enumerate(tqdm(test_data, desc=\"Zero-Shot CoT\")):\n",
        "        question = example['question']\n",
        "        ground_truth = example['answer']\n",
        "\n",
        "        prompt = ZERO_SHOT_COT_PROMPT.format(question=question)\n",
        "        response = generate_response(model, tokenizer, prompt)\n",
        "\n",
        "        is_correct = check_answer_correctness(response, ground_truth)\n",
        "        if is_correct:\n",
        "            correct += 1\n",
        "\n",
        "        results.append({\n",
        "            'question': question,\n",
        "            'ground_truth': ground_truth,\n",
        "            'response': response,\n",
        "            'correct': is_correct\n",
        "        })\n",
        "\n",
        "        if (i + 1) % 50 == 0:\n",
        "            print(f\"Progress: {i+1}/{total}, Accuracy: {correct/(i+1)*100:.2f}%\")\n",
        "\n",
        "    accuracy = correct / total * 100\n",
        "    print(f\"\\nZero-Shot CoT Accuracy: {accuracy:.2f}% ({correct}/{total})\")\n",
        "\n",
        "    # Save results\n",
        "    with open(os.path.join(config.OUTPUT_DIR, 'zero_shot_cot_results.json'), 'w') as f:\n",
        "        json.dump({'accuracy': accuracy, 'correct': correct, 'total': total, 'results': results}, f, indent=2)\n",
        "\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vanilla_data"
      },
      "outputs": [],
      "source": [
        "# Data Preparation for Vanilla SFT\n",
        "def prepare_vanilla_sft_data(train_data):\n",
        "\n",
        "    print(\"\\nPreparing Vanilla SFT dataset...\")\n",
        "\n",
        "    sft_data = []\n",
        "    for example in tqdm(train_data, desc=\"Processing training data\"):\n",
        "        question = example['question']\n",
        "        answer = example['answer']  # Contains reasoning + #### + final answer\n",
        "\n",
        "        # Create training example\n",
        "        prompt = f\"Question: {question}\\nLet's think step by step to solve this problem.\\n\"\n",
        "        completion = answer\n",
        "\n",
        "        sft_data.append({\n",
        "            'prompt': prompt,\n",
        "            'completion': completion,\n",
        "            'text': prompt + completion\n",
        "        })\n",
        "\n",
        "    print(f\"Prepared {len(sft_data)} training examples for Vanilla SFT\")\n",
        "\n",
        "    # Save data\n",
        "    with open(os.path.join(config.DATA_DIR, 'vanilla_sft_data.json'), 'w') as f:\n",
        "        json.dump(sft_data, f, indent=2)\n",
        "\n",
        "    return sft_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "star_data_gen"
      },
      "outputs": [],
      "source": [
        "# STaR Data Generation\n",
        "def generate_star_dataset(model, tokenizer, train_data, iteration: int = 0):\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*50)\n",
        "    print(f\"Generating STaR Dataset - Iteration {iteration}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    star_data = []\n",
        "    stats = {\n",
        "        'correct_first_attempt': 0,\n",
        "        'correct_with_hint': 0,\n",
        "        'total': len(train_data)\n",
        "    }\n",
        "\n",
        "    for i, example in enumerate(tqdm(train_data, desc=f\"STaR Iteration {iteration}\")):\n",
        "        question = example['question']\n",
        "        ground_truth = example['answer']\n",
        "        gt_answer = extract_answer(ground_truth)\n",
        "\n",
        "        # Step 1: Generate rationale without hint\n",
        "        prompt_no_hint = PROMPT_WITHOUT_HINT.format(question=question)\n",
        "        rationale_no_hint = generate_response(model, tokenizer, prompt_no_hint)\n",
        "\n",
        "        # Check if answer is correct\n",
        "        is_correct = check_answer_correctness(rationale_no_hint, ground_truth)\n",
        "\n",
        "        if is_correct:\n",
        "            # Answer is correct, use this rationale\n",
        "            star_data.append({\n",
        "                'prompt': prompt_no_hint,\n",
        "                'completion': rationale_no_hint,\n",
        "                'text': prompt_no_hint + '\\n' + rationale_no_hint,\n",
        "                'source': 'correct_first_attempt'\n",
        "            })\n",
        "            stats['correct_first_attempt'] += 1\n",
        "        else:\n",
        "            # Step 2: Answer is wrong, generate with hint (rationalization)\n",
        "            prompt_with_hint = PROMPT_WITH_HINT.format(question=question, answer=gt_answer)\n",
        "            rationale_with_hint = generate_response(model, tokenizer, prompt_with_hint)\n",
        "\n",
        "            # CRITICAL: Train as if the model generated this without the hint\n",
        "            star_data.append({\n",
        "                'prompt': PROMPT_WITHOUT_HINT.format(question=question),\n",
        "                'completion': rationale_with_hint,\n",
        "                'text': PROMPT_WITHOUT_HINT.format(question=question) + '\\n' + rationale_with_hint,\n",
        "                'source': 'rationalization'\n",
        "            })\n",
        "            stats['correct_with_hint'] += 1\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"Progress: {i+1}/{len(train_data)}\")\n",
        "            print(f\"  Correct first: {stats['correct_first_attempt']}, With hint: {stats['correct_with_hint']}\")\n",
        "\n",
        "    print(f\"\\nSTaR Dataset Generation Complete!\")\n",
        "    print(f\"Total examples: {len(star_data)}\")\n",
        "    print(f\"Statistics: {stats}\")\n",
        "\n",
        "    # Save dataset\n",
        "    save_path = os.path.join(config.DATA_DIR, f'star_data_iter_{iteration}.json')\n",
        "    with open(save_path, 'w') as f:\n",
        "        json.dump({'data': star_data, 'stats': stats}, f, indent=2)\n",
        "\n",
        "    return star_data, stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_func"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "def prepare_dataset_for_training(data_list, tokenizer):\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        # Tokenize the full text (prompt + completion)\n",
        "        model_inputs = tokenizer(\n",
        "            examples['text'],\n",
        "            max_length=config.MAX_LENGTH,\n",
        "            truncation=True,\n",
        "            padding='max_length'\n",
        "        )\n",
        "\n",
        "        # Labels are the same as input_ids for causal LM\n",
        "        model_inputs['labels'] = model_inputs['input_ids'].copy()\n",
        "\n",
        "        return model_inputs\n",
        "\n",
        "    # Convert to dataset format\n",
        "    dataset = Dataset.from_dict({\n",
        "        'text': [item['text'] for item in data_list]\n",
        "    })\n",
        "\n",
        "    # Tokenize\n",
        "    tokenized_dataset = dataset.map(\n",
        "        tokenize_function,\n",
        "        batched=True,\n",
        "        remove_columns=dataset.column_names,\n",
        "        desc=\"Tokenizing dataset\"\n",
        "    )\n",
        "\n",
        "    return tokenized_dataset\n",
        "\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "def train_model(train_data, output_dir, model_name=\"model\"):\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*50)\n",
        "    print(f\"Training {model_name}...\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Load fresh model from base checkpoint\n",
        "    # Load model with 8-bit quantization\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        config.MODEL_NAME,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map={'': 0},\n",
        "        load_in_8bit=True\n",
        "    )\n",
        "\n",
        "    # Configure PEFT (LoRA)\n",
        "    lora_config = LoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        target_modules=[\"q_proj\", \"v_proj\"],\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "\n",
        "    # Get PEFT model\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "\n",
        "    # Prepare dataset\n",
        "    tokenized_dataset = prepare_dataset_for_training(train_data, tokenizer)\n",
        "    print(f\"Training on {len(tokenized_dataset)} examples\")\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=config.NUM_EPOCHS,\n",
        "        per_device_train_batch_size=config.BATCH_SIZE,\n",
        "        gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,\n",
        "        learning_rate=config.LEARNING_RATE,\n",
        "        warmup_steps=config.WARMUP_STEPS,\n",
        "        logging_steps=50,\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=2,\n",
        "        fp16=True,\n",
        "        report_to=\"none\",\n",
        "        remove_unused_columns=False,\n",
        "    )\n",
        "\n",
        "    # Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "        data_collator=DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    print(f\"Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # Save model\n",
        "    trainer.save_model(output_dir)\n",
        "    print(f\"Model saved to {output_dir}\")\n",
        "\n",
        "    # Clear memory\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return trainer.model # Return the trained PEFT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eval_func"
      },
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "def evaluate_model(model, tokenizer, test_data, model_name=\"model\", num_samples=None):\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*50)\n",
        "    print(f\"Evaluating {model_name}...\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    if num_samples:\n",
        "        test_data = test_data.select(range(min(num_samples, len(test_data))))\n",
        "\n",
        "    correct = 0\n",
        "    total = len(test_data)\n",
        "    results = []\n",
        "\n",
        "    for i, example in enumerate(tqdm(test_data, desc=f\"Evaluating {model_name}\")):\n",
        "        question = example['question']\n",
        "        ground_truth = example['answer']\n",
        "\n",
        "        prompt = PROMPT_WITHOUT_HINT.format(question=question)\n",
        "        response = generate_response(model, tokenizer, prompt)\n",
        "\n",
        "        is_correct = check_answer_correctness(response, ground_truth)\n",
        "        if is_correct:\n",
        "            correct += 1\n",
        "\n",
        "        results.append({\n",
        "            'question': question,\n",
        "            'ground_truth': ground_truth,\n",
        "            'response': response,\n",
        "            'correct': is_correct\n",
        "        })\n",
        "\n",
        "        if (i + 1) % 50 == 0:\n",
        "            print(f\"Progress: {i+1}/{total}, Accuracy: {correct/(i+1)*100:.2f}%\")\n",
        "\n",
        "    accuracy = correct / total * 100\n",
        "    print(f\"\\n{model_name} Accuracy: {accuracy:.2f}% ({correct}/{total})\")\n",
        "\n",
        "    return accuracy, results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_zero_shot"
      },
      "outputs": [],
      "source": [
        "# Evaluate Zero-Shot CoT\n",
        "zero_shot_accuracy = evaluate_zero_shot_cot(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    test_dataset,\n",
        "    num_samples=config.TEST_SAMPLE_SIZE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_vanilla",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Prepare Vanilla SFT data\n",
        "vanilla_sft_data = prepare_vanilla_sft_data(train_dataset)\n",
        "\n",
        "# Train Vanilla SFT model\n",
        "vanilla_model = train_model(\n",
        "    vanilla_sft_data,\n",
        "    config.VANILLA_SFT_DIR,\n",
        "    model_name=\"Vanilla SFT\"\n",
        ")\n",
        "\n",
        "# Evaluate Vanilla SFT\n",
        "vanilla_accuracy, vanilla_results = evaluate_model(\n",
        "    vanilla_model,\n",
        "    tokenizer,\n",
        "    test_dataset,\n",
        "    model_name=\"Vanilla SFT\",\n",
        "    num_samples=config.TEST_SAMPLE_SIZE\n",
        ")\n",
        "\n",
        "# Save results\n",
        "with open(os.path.join(config.OUTPUT_DIR, 'vanilla_sft_results.json'), 'w') as f:\n",
        "    json.dump({\n",
        "        'accuracy': vanilla_accuracy,\n",
        "        'correct': sum(1 for r in vanilla_results if r['correct']),\n",
        "        'total': len(vanilla_results),\n",
        "        'results': vanilla_results\n",
        "    }, f, indent=2)\n",
        "\n",
        "# Clear memory\n",
        "del vanilla_model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_star",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# STaR Iterative Training\n",
        "star_results = []\n",
        "current_model = model  # Start with base model\n",
        "\n",
        "for iteration in range(config.STAR_ITERATIONS):\n",
        "    print(f\"\\n\\n{'='*60}\")\n",
        "    print(f\"STaR ITERATION {iteration + 1}/{config.STAR_ITERATIONS}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    # Step 1: Generate STaR dataset using current model\n",
        "    star_data, stats = generate_star_dataset(\n",
        "        current_model,\n",
        "        tokenizer,\n",
        "        train_dataset,\n",
        "        iteration=iteration\n",
        "    )\n",
        "\n",
        "    # Step 2: Train on the generated dataset\n",
        "    iter_output_dir = os.path.join(config.STAR_DIR, f\"iteration_{iteration}\")\n",
        "    new_model = train_model(\n",
        "        star_data,\n",
        "        iter_output_dir,\n",
        "        model_name=f\"STaR Iteration {iteration}\"\n",
        "    )\n",
        "\n",
        "    # Clear previous model from memory\n",
        "    if iteration > 0:\n",
        "        del current_model\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    current_model = new_model\n",
        "\n",
        "    # Step 3: Evaluate\n",
        "    accuracy, results = evaluate_model(\n",
        "        current_model,\n",
        "        tokenizer,\n",
        "        test_dataset,\n",
        "        model_name=f\"STaR Iteration {iteration}\",\n",
        "        num_samples=config.TEST_SAMPLE_SIZE\n",
        "    )\n",
        "\n",
        "    star_results.append({\n",
        "        'iteration': iteration,\n",
        "        'accuracy': accuracy,\n",
        "        'stats': stats,\n",
        "        'results': results\n",
        "    })\n",
        "\n",
        "    # Save iteration results\n",
        "    with open(os.path.join(config.OUTPUT_DIR, f'star_iteration_{iteration}_results.json'), 'w') as f:\n",
        "        json.dump(star_results[-1], f, indent=2)\n",
        "\n",
        "    print(f\"\\nâœ“ Iteration {iteration} completed!\")\n",
        "    print(f\"  Accuracy: {accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "final_results"
      },
      "outputs": [],
      "source": [
        "# Final results\n",
        "final_results = {\n",
        "    'Zero-Shot CoT': zero_shot_accuracy,\n",
        "    'Vanilla SFT': vanilla_accuracy,\n",
        "    'STaR': [result['accuracy'] for result in star_results]\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL RESULTS - GSM8K Test Set\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nTest samples: {config.TEST_SAMPLE_SIZE if config.TEST_SAMPLE_SIZE else 'Full test set'}\")\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "print(f\"Zero-Shot CoT:  {final_results['Zero-Shot CoT']:>6.2f}%\")\n",
        "print(f\"Vanilla SFT:    {final_results['Vanilla SFT']:>6.2f}%\")\n",
        "for i, acc in enumerate(final_results['STaR']):\n",
        "    print(f\"STaR Iter {i}:   {acc:>6.2f}%\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "# Create results table\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Results Table (Exact Match Accuracy)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\n| Method              | Accuracy (%) |\")\n",
        "print(f\"|---------------------|--------------|\")\n",
        "print(f\"| Zero-Shot CoT       | {final_results['Zero-Shot CoT']:>12.2f} |\")\n",
        "print(f\"| Vanilla SFT         | {final_results['Vanilla SFT']:>12.2f} |\")\n",
        "for i, acc in enumerate(final_results['STaR']):\n",
        "    print(f\"| STaR Iteration {i}   | {acc:>12.2f} |\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Save final results\n",
        "with open(os.path.join(config.OUTPUT_DIR, 'final_results.json'), 'w') as f:\n",
        "    json.dump(final_results, f, indent=2)\n",
        "\n",
        "print(f\"All results saved to {config.OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot_results"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot results\n",
        "methods = ['Zero-Shot CoT', 'Vanilla SFT'] + [f'STaR Iter {i}' for i in range(len(final_results['STaR']))]\n",
        "accuracies = [final_results['Zero-Shot CoT'], final_results['Vanilla SFT']] + final_results['STaR']\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "colors = ['#3498db', '#2ecc71'] + ['#e74c3c'] * len(final_results['STaR'])\n",
        "bars = plt.bar(methods, accuracies, color=colors, alpha=0.8, edgecolor='black')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "             f'{height:.2f}%',\n",
        "             ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.xlabel('Method', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
        "plt.title('GSM8K Test Set Performance Comparison', fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(config.OUTPUT_DIR, 'results_comparison.png'), dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Plot STaR progression\n",
        "if len(final_results['STaR']) > 1:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    iterations = list(range(len(final_results['STaR'])))\n",
        "    plt.plot(iterations, final_results['Zero-Shot CoT'], marker='o', linewidth=2, markersize=10, color='#e74c3c')\n",
        "    plt.axhline(y=final_results['Vanilla SFT'], color='#2ecc71', linestyle='--', label='Vanilla SFT', linewidth=2)\n",
        "    plt.axhline(y=final_results['STaR'], color='#3498db', linestyle='--', label='Zero-Shot CoT', linewidth=2)\n",
        "\n",
        "    plt.xlabel('STaR Iteration', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
        "    plt.title('STaR Accuracy Progression', fontsize=14, fontweight='bold')\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.grid(alpha=0.3, linestyle='--')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(config.OUTPUT_DIR, 'star_progression.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2I80HCVRz_0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}